spring:
  application:
    name: ai-code-backend
  #session配置 注:这是指后端的session过期时间
  session:
    store-type: redis #session存在redis里
    timeout: 2592000 #30天过期

  #mysql
  datasource:
    driver-class-name: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://localhost:3306/ai_code
    username: root
    password: 123456
  #redis
  data:
    redis:
      host: localhost
      port: 6379
      password:
      ttl: 3600 # seconds,该配置表示key的默认过期时间
      database: 0
  profiles:
    active: local
server:
  port: 8123
  servlet:
    context-path: /api
    session:
      cookie:
        max-age: 2592000 #chookie30天过期 最好要和session过期时间一致.注:这是前端的
#springdoc-openapi
springdoc:
  group-configs:
    - group: 'default'
      paths-to-match: '/**'
      packages-to-scan: cn.iamwsll.aicode.controller
# knife4j
knife4j:
  enable: true
  setting:
    language: zh_cn

#监控
management:
  endpoints:
    web:
      exposure:
        include: health,info,prometheus
  endpoint:
    health:
      show-details: always


#以下是application-local.yml应该有的内容.不注释也会使用application-local.yml的.

#langchain4j:
#  open-ai:
#    chat-model:
#      base-url: https://api.siliconflow.cn/v1
#      api-key: your api key
#      model-name: deepseek-ai/DeepSeek-V3.2-Exp
#      log-requests: true
#      log-responses: true
#      max-tokens: 8192 # 模型输出最大长度,避免json截断.如果不设置,那么默认4096
#    #      strict-json-schema: true
#    #      response-format: json_object #这两条不能加,因为只需要返回枚举类型时,这里开启结构化输出就会报错.(这是因为结构化输出不兼容某些类型,比如枚举类)
#
#    # 智能路由 AI 模型配置（用于简单的分类任务）
#    routing-chat-model:
#      base-url: https://api.siliconflow.cn/v1
#      api-key: your api key
#      model-name: Qwen/Qwen3-30B-A3B-Instruct-2507
#    #      log-requests: true
#    #      log-responses: true
#    streaming-chat-model:
#      base-url: https://api.siliconflow.cn/v1
#      api-key: your api key
#      model-name: Qwen/Qwen3-Coder-30B-A3B-Instruct
#      max-tokens: 262143 #其实按照手册,这个字段不要设置.但从测试结果来看,这个参数设置+设置响应超时时间更长,极其有利于避免回答被截断和框架报read timeout
#      log-requests: true
#      log-responses: true
#    # 推理 AI 模型配置（用于复杂的推理任务）
#    reasoning-streaming-chat-model:
#      base-url: https://api.siliconflow.cn/v1
#      api-key: your api key
#      model-name: Qwen/Qwen3-Coder-30B-A3B-Instruct
#      max-tokens: 262143 #其实按照手册,这个字段不要设置.但从测试结果来看,这个参数设置+设置响应超时时间更长,极其有利于避免回答被截断和框架报read timeout
#      temperature: 0.1
#      log-requests: true
#      log-responses: true
#
#
#aliyun:
#  oss:
#    endpoint: oss-cn-hangzhou.aliyuncs.com
#    access-key-id: your access key id
#    access-key-secret: your access key secret
#    bucket-name: wsll-ai-code
#    region: cn-hangzhou
#
#pexels:
#  api-key: your pexels api key
#
#dashscope:
#  api-key: your dashscope api key
#  image-model: wan2.2-t2i-flash




# 以下则是经过测试的一些可用模型配置示例,可以直接使用.

#    chat-model:
#      base-url: https://api.deepseek.com
#      api-key: your api key
#      model-name: deepseek-chat
#      log-requests: true
#      log-responses: true
#      max-tokens: 8192 # 模型输出最大长度,避免json截断.如果不设置,那么默认4096
#      strict-json-schema: true #deepseek模型开启严格json schema校验的方式
#      response-format: json_object # deepseek模型开启严格json schema校验的方式

#    streaming-chat-model:
#      base-url: https://api.moonshot.cn/v1
#      api-key: your api key
#      model-name: kimi-k2-turbo-preview
##      max-tokens: 8192
#      log-requests: true
#      log-responses: true
#    streaming-chat-model:
#      base-url: https://api.deepseek.com
#      api-key: your api key
#      model-name: deepseek-chat
#      max-tokens: 8192
#      log-requests: true
#      log-responses: true
#    streaming-chat-model:
#      base-url: https://api.siliconflow.cn/v1
#      api-key: your api key
#      model-name: deepseek-ai/DeepSeek-V3.2-Exp
#      max-tokens: 8192
#      log-requests: true
#      log-responses: true
#    # 推理 AI 模型配置（用于复杂的推理任务）
#    reasoning-streaming-chat-model:
#      base-url: https://api.siliconflow.cn/v1
#      api-key: your api key
#      model-name: deepseek-ai/DeepSeek-V3.2-Exp
#      max-tokens: 8192
#      temperature: 0.1
#      log-requests: true
#      log-responses: true